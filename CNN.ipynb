{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTE6HUY0ITRz59Wyd6zi1w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kashara-Alvin-Ssali/Machine-Learning/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HJY1cxKghQzG"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define path to Dataset.zip\n",
        "zip_path = \"/content/drive/My Drive/Dataset.zip\"\n",
        "\n",
        "# Extract Dataset.zip\n",
        "extract_path = \"/content/Dataset\"\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Dataset extracted successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qQJQ-otmP42",
        "outputId": "0c60ab5a-d133-420f-9f06-4afce569a90f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Dataset extracted successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Image data generator for training, validation, and testing\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "\n",
        "# Define data_dir\n",
        "data_dir = \"/content/Dataset/Dataset\" # Assuming your dataset is in '/content/Dataset'\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    os.path.join(data_dir, 'Training'),\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='binary')\n",
        "\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "    os.path.join(data_dir, 'Validation'),\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='binary')\n",
        "\n",
        "test_generator = datagen.flow_from_directory(\n",
        "    os.path.join(data_dir, 'Testing'),\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjfIqdIBmXeE",
        "outputId": "89839936-cfe2-42fc-c41b-bd4db98ffb3f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 62 images belonging to 2 classes.\n",
            "Found 18 images belonging to 2 classes.\n",
            "Found 18 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "base_model.trainable = True  # Freeze base layers\n",
        "\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')  # Binary classification\n",
        "])\n"
      ],
      "metadata": {
        "id": "-otPgctHmYOP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # Added compile step\n",
        "model.fit(train_generator, epochs=20, validation_data=validation_generator)\n",
        "\n",
        "# Save model\n",
        "model.save('currency_detector.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9B5Qlo2mdLK",
        "outputId": "c03e728a-f60d-47b7-894e-90ed31a3b470"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 5s/step - accuracy: 0.6277 - loss: 0.7016 - val_accuracy: 0.5556 - val_loss: 0.9797\n",
            "Epoch 2/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.7502 - loss: 0.5070 - val_accuracy: 0.5556 - val_loss: 1.4493\n",
            "Epoch 3/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3s/step - accuracy: 0.8269 - loss: 0.5717 - val_accuracy: 0.5000 - val_loss: 0.9542\n",
            "Epoch 4/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.7717 - loss: 0.7247 - val_accuracy: 0.6667 - val_loss: 0.8302\n",
            "Epoch 5/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 0.7505 - loss: 0.6344 - val_accuracy: 0.4444 - val_loss: 0.7285\n",
            "Epoch 6/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3s/step - accuracy: 0.8484 - loss: 0.4226 - val_accuracy: 0.6111 - val_loss: 0.6787\n",
            "Epoch 7/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.8480 - loss: 0.3172 - val_accuracy: 0.5556 - val_loss: 0.8576\n",
            "Epoch 8/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 0.8296 - loss: 0.3895 - val_accuracy: 0.5556 - val_loss: 0.8827\n",
            "Epoch 9/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3s/step - accuracy: 0.8511 - loss: 0.3274 - val_accuracy: 0.5000 - val_loss: 0.8723\n",
            "Epoch 10/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2s/step - accuracy: 0.8373 - loss: 0.3355 - val_accuracy: 0.6111 - val_loss: 0.8484\n",
            "Epoch 11/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3s/step - accuracy: 0.7977 - loss: 0.4024 - val_accuracy: 0.4444 - val_loss: 2.1094\n",
            "Epoch 12/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 0.8616 - loss: 0.2635 - val_accuracy: 0.4444 - val_loss: 2.4639\n",
            "Epoch 13/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 0.8158 - loss: 0.4899 - val_accuracy: 0.4444 - val_loss: 2.9146\n",
            "Epoch 14/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3s/step - accuracy: 0.7981 - loss: 0.4564 - val_accuracy: 0.4444 - val_loss: 3.0760\n",
            "Epoch 15/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 0.8088 - loss: 0.4546 - val_accuracy: 0.4444 - val_loss: 4.0975\n",
            "Epoch 16/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 0.8914 - loss: 0.3401 - val_accuracy: 0.4444 - val_loss: 5.3068\n",
            "Epoch 17/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3s/step - accuracy: 0.8803 - loss: 0.2449 - val_accuracy: 0.4444 - val_loss: 6.1348\n",
            "Epoch 18/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 0.8477 - loss: 0.2593 - val_accuracy: 0.4444 - val_loss: 6.5966\n",
            "Epoch 19/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 0.8480 - loss: 0.2559 - val_accuracy: 0.4444 - val_loss: 7.7090\n",
            "Epoch 20/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3s/step - accuracy: 0.8508 - loss: 0.2641 - val_accuracy: 0.4444 - val_loss: 8.2058\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and test a new image\n",
        "def preprocess_image(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    img = cv2.resize(img, (128, 128))\n",
        "    img = img / 255.0\n",
        "    img = np.expand_dims(img, axis=[0, -1])\n",
        "    return img\n",
        "\n",
        "def predict_currency(image_path, model):\n",
        "    img = preprocess_image(image_path)\n",
        "    prediction = model.predict(img)[0][0]\n",
        "    return 'Real' if prediction < 0.5 else 'Fake'\n",
        "\n",
        "# Feature extraction - HSV decomposition\n",
        "def extract_hsv_features(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "    h, s, v = cv2.split(hsv_img)\n",
        "    return h, s, v\n"
      ],
      "metadata": {
        "id": "s4_PoVUImgG1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Edge detection\n",
        "def detect_edges(image_path):\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    edges = cv2.Canny(img, 100, 200)\n",
        "    return edges\n",
        "\n",
        "# Evaluate on test data\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print(f'Test Accuracy: {test_acc * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTl0z5Ibmi-E",
        "outputId": "745e3cac-e279-4c11-b586-2269c798a6d3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498ms/step - accuracy: 0.8333 - loss: 1.9250\n",
            "Test Accuracy: 83.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# ... (Your existing code) ...\n",
        "\n",
        "# Load and test a new image\n",
        "def preprocess_image(image_path):\n",
        "    \"\"\"Preprocesses the image for prediction.\"\"\"\n",
        "    # Check if the image file exists\n",
        "    if not os.path.exists(image_path):\n",
        "        raise FileNotFoundError(f\"Image file not found: {image_path}\")\n",
        "\n",
        "    img = cv2.imread(image_path)\n",
        "\n",
        "    # Check if the image was loaded successfully\n",
        "    if img is None:\n",
        "        raise IOError(f\"Failed to load image: {image_path}\")\n",
        "\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "    img = cv2.resize(img, (128, 128))\n",
        "    img = img / 255.0  # Normalize pixel values\n",
        "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
        "    return img\n",
        "\n",
        "def predict_currency(image_path, model):\n",
        "    \"\"\"Predicts the currency class of the image.\"\"\"\n",
        "    img = preprocess_image(image_path)\n",
        "    prediction = model.predict(img)[0][0]\n",
        "    return 'Real' if prediction < 0.5 else 'Fake'\n",
        "\n",
        "# Example usage:\n",
        "image_path = '/content/Dataset/Dataset/Testing/Fake/Fake.jpeg'  # Replace with the path to your image\n",
        "# Check if the image file exists\n",
        "if not os.path.exists(image_path):\n",
        "    raise FileNotFoundError(f\"Image file not found: {image_path}\")\n",
        "\n",
        "prediction = predict_currency(image_path, model)\n",
        "print(f\"Prediction for {image_path}: {prediction}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGQXu9zP4Z3s",
        "outputId": "310bde73-9a18-4462-b0bb-c9c30d60e628"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Prediction for /content/Dataset/Dataset/Testing/Fake/Fake.jpeg: Real\n"
          ]
        }
      ]
    }
  ]
}